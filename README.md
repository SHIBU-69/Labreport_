#  Diabetes Prediction using Machine Learning

This project demonstrates how to build a Logistic Regression model to predict whether a patient has diabetes based on medical attributes. It uses the Pima Indian Diabetes dataset and is designed for Google Colab, including integration with Google Drive.



# Dataset

The dataset used is the **Pima Indians Diabetes Database**. It contains 768 entries with 8 clinical features and 1 binary outcome:

- `Pregnancies`
- `Glucose`
- `BloodPressure`
- `SkinThickness`
- `Insulin`
- `BMI`
- `DiabetesPedigreeFunction`
- `Age`
- `Outcome` (0 = No diabetes, 1 = Diabetes)

Location in Google Drive:  
`MyDrive/CSV FILE/diabetes.csv`



# Objectives

- Load dataset from Google Drive
- Handle missing values and perform EDA
- Train a Logistic Regression model
- Evaluate performance using classification metrics



# Technologies Used

- Python
- Pandas, NumPy
- Matplotlib, Seaborn
- Scikit-learn
- Google Colab
- Google Drive


# How to Run

# In Google Colab
1. Mount your Google Drive:
   
from google.colab import drive
drive.mount('/content/drive')

2. Load the dataset:
csv_path = '/content/drive/MyDrive/CSV FILE/diabetes.csv'
df = pd.read_csv(csv_path)

3. Run the notebook cells for EDA, cleaning, training, and evaluation.
# KNN from Scratch: Flower and News Classification with Custom Evaluation Metrics

## ğŸ“Œ Project Title
**KNN from Scratch: Flower and News Classification with Custom Evaluation Metrics**

## ğŸ§  Overview
This project implements the **K-Nearest Neighbors (KNN)** algorithm from scratch using Python. It also includes the development of custom evaluation metrics such as accuracy, confusion matrix, precision, recall, and F1-score.

The algorithm is tested on the **Wisconsin Breast Cancer Dataset** (CSV file: `KNNAlgorithmDataset.csv`). The aim is to classify tumors as benign or malignant, optimize model performance, and compare the custom implementation with scikit-learn's `KNeighborsClassifier`.

## ğŸ¯ Objectives
- âœ… Implement KNN algorithm from scratch (without ML libraries)
- âœ… Build custom evaluation metrics:
  - Accuracy
  - Confusion Matrix
  - Precision
  - Recall
  - F1-Score
- âœ… Apply the algorithm to a real dataset
- âœ… Optimize hyperparameter `k` and train-test split ratio
- âœ… Compare with `sklearn` implementation
- âœ… Document mathematical theory behind KNN and metrics

## ğŸ§ª Procedure
1. **Theoretical Foundation**:
   - Review Euclidean distance formula
   - Define evaluation metric formulas using TP, TN, FP, FN

2. **KNN Class Implementation**:
   - `fit(X, y)` â€“ store training data
   - `predict(X)` â€“ classify test points using majority vote of `k` nearest neighbors

3. **Metric Functions**:
   - Custom functions for all evaluation metrics

4. **Data Preprocessing**:
   - Drop unused columns (`id`, `Unnamed: 32`)
   - Encode target (`M`â†’1, `B`â†’0)
   - Scale features using `StandardScaler`

5. **Model Optimization**:
   - Find best value of `k` (1 to 25)
   - Try various train-test split ratios (e.g., 80/20, 75/25)

6. **Final Comparison**:
   - Train both custom and sklearn KNN on same data
   - Evaluate using all metrics

## ğŸ“‚ Files
- `KNNAlgorithmDataset.csv` â€“ Breast Cancer Dataset
- `knn_from_scratch.ipynb` â€“ Jupyter Notebook implementation
- `LabReport02-knnFromScratch.pdf` â€“ Project report

## ğŸ“ˆ Output Summary
- **Accuracy**: 95.6%
- **Precision**: 92.9%
- **Recall**: 95.3%
- **F1 Score**: 94.1%

The custom KNN model performs nearly identically to scikit-learnâ€™s, proving the correctness and effectiveness of the implementation.

## ğŸ§‘â€ğŸ’» Author
- **Name**: Al Motakabbir Mahmud Shihab  
- **ID**: 222002061  
- **University**: Green University of Bangladesh  
- **Course**: CSE 412 (Machine Learning Lab), Section: 222 D3  

## ğŸ”— GitHub Repository
[ğŸ”— GitHub Notebook Link](https://github.com/SHIBU-69/Labreport_/blob/main/222002061_CSE312_222D3_LabReport02_knnFromScratch.ipynb)
